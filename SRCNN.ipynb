{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS-474-Final-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w02HK43C9m7Z",
        "outputId": "c1ea86c7-2260-4172-cd6e-fdcdec188637"
      },
      "source": [
        "!pip3 install torch\n",
        "!pip3 install torchvision\n",
        "!pip3 install tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm import tqdm\n",
        "import tqdm.notebook as tq\n",
        "from torch.nn.parameter import Parameter\n",
        "import pdb\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "# File Operations\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import gzip\n",
        "import tarfile\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "# Image Operations\n",
        "import cv2 as cv \n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDz3TJ6Y81F-"
      },
      "source": [
        "class CelebASuperResolutionDataset(Dataset):\n",
        "  def __init__(self, root, download_root, download=False, train=True):\n",
        "    # Directory Names\n",
        "    folder_dir = \"celeba\"\n",
        "    train_dir = \"train\"\n",
        "    validation_dir = \"validation\"\n",
        "\n",
        "    # Paths\n",
        "    folder_path = os.path.join(root, folder_dir)\n",
        "    \n",
        "    # Download\n",
        "    if download and not os.path.exists(download_root):\n",
        "      datasets.utils.download_url('https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip', download_root, folder_dir + '.zip', None)\n",
        "\n",
        "    # Unzip Dataset\n",
        "    if download and not os.path.exists(folder_path):\n",
        "      os.mkdir(folder_path)\n",
        "      with zipfile.ZipFile(os.path.join(download_root, folder_dir + '.zip'), 'r') as zip_ref:\n",
        "        zip_ref.extractall(folder_path)\n",
        "\n",
        "    # Create Train Path\n",
        "    train_path = os.path.join(folder_path, train_dir)\n",
        "    if not os.path.exists(train_path):\n",
        "      os.mkdir(train_path)\n",
        "      os.mkdir(os.path.join(train_path, 'train'))\n",
        "\n",
        "    # Create Validation Path\n",
        "    validation_path = os.path.join(folder_path, validation_dir)\n",
        "    if not os.path.exists(validation_path):\n",
        "      os.mkdir(validation_path)\n",
        "      os.mkdir(os.path.join(validation_path, 'validation'))\n",
        "\n",
        "    # Split Training and Validation\n",
        "    if download:\n",
        "      validation_ratio = 0.05\n",
        "      file_names = os.listdir(os.path.join(folder_path, 'img_align_celeba'))\n",
        "      \n",
        "      # Move Last 20% of Image to Validation Folder\n",
        "      # last_n = int(len(file_names) - len(file_names) * validation_ratio)\n",
        "      last_n = len(file_names) - 500\n",
        "      for i, file_name in enumerate(file_names):\n",
        "        old_file_path = os.path.join(folder_path, 'img_align_celeba', file_name)\n",
        "        new_file_path = old_file_path\n",
        "        if i < last_n:\n",
        "          new_file_path = os.path.join(train_path, 'train', file_name)\n",
        "        else:\n",
        "          new_file_path = os.path.join(validation_path, 'validation', file_name)\n",
        "\n",
        "        shutil.move(old_file_path, new_file_path)\n",
        "\n",
        "    h = 218\n",
        "    w = 178\n",
        "    # Image Folder\n",
        "    source_path = train_path if train else validation_path\n",
        "    self.blurred_folder = torchvision.datasets.ImageFolder(source_path,\n",
        "                                                           transform=transforms.Compose([transforms.Resize((h//4, w//4)),\n",
        "                                                                                         transforms.Resize((h, w)),\n",
        "                                                                                         transforms.ToTensor()]))\n",
        "    self.original_folder = torchvision.datasets.ImageFolder(source_path,\n",
        "                                                            transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    blurred = self.blurred_folder[index]\n",
        "    original = self.original_folder[index]\n",
        "    return blurred[0], original[0]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.blurred_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4ffWWbns3JG"
      },
      "source": [
        "class SRCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SRCNN, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(9,9), padding=9//2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(3, 3), padding=3//2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=3, kernel_size=(5,5), padding=5//2)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.net(self.net(x).squeeze(2).squeeze(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziK-vuzO5VQy"
      },
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 50\n",
        "NUM_WORKERS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C42CR_0s8Unc"
      },
      "source": [
        "# Image Paths\n",
        "root = '/content'\n",
        "download_root = '/drive/MyDrive/CS-474-Final-Project'\n",
        "\n",
        "# Datasets\n",
        "train_dataset = CelebASuperResolutionDataset(root, download_root, train=True, download=False)\n",
        "val_dataset = CelebASuperResolutionDataset(root, download_root, train=False)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          pin_memory=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        shuffle=True,\n",
        "                        num_workers=NUM_WORKERS,\n",
        "                        pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEnnFp5KLd1Q"
      },
      "source": [
        "# Network Details\n",
        "def train():\n",
        "  gc.collect()\n",
        "  # model = torch.hub.load('pytorch/vision:v0.9.0', 'resnet50', pretrained=False).cuda()\n",
        "  model = SRCNN().cuda()\n",
        "  # objective = nn.CrossEntropyLoss()\n",
        "  objective = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "  # Train\n",
        "  losses = []\n",
        "  validations = []\n",
        "  accuracies = []\n",
        "  predictions = []\n",
        "  accuracy = 0\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    loop = tqdm(total=len(train_loader), position=0, leave=False)\n",
        "    for batch_num, (x, y_truth) in enumerate(train_loader):\n",
        "      gc.collect()\n",
        "      # Reset Gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Pass Blurred Image Through Model\n",
        "      x, y_truth = x.cuda(non_blocking=True), y_truth.cuda(non_blocking=True)\n",
        "      y_hat = model(x)\n",
        "\n",
        "      # Calculate Loss\n",
        "      loss = objective(y_hat, y_truth)\n",
        "      loss.backward()\n",
        "\n",
        "      # Optimize\n",
        "      optimizer.step()\n",
        "\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      loop.set_description('epoch:{}, loss:{:.4f}, accuracy:{:.3f}'.format(epoch, loss, accuracy))\n",
        "      loop.update(1)\n",
        "\n",
        "      if (batch_num + 1) % 200 == 0:\n",
        "        for x, y in val_loader:\n",
        "          # Validations\n",
        "          val = np.mean([objective(model(x.cuda()), y.cuda()).item() for x, y in val_loader])\n",
        "          validations.append((len(losses), val))\n",
        "\n",
        "    # Prediction\n",
        "    val_72 = val_dataset[172][0].unsqueeze(0).cuda()\n",
        "    prediction = model(val_72).squeeze(0)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "  loop.close()\n",
        "  gc.collect()\n",
        "  return model, losses, validations, predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiY4pnwxcXu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e7c867-2a05-48ce-c997-4ffa5c46c347"
      },
      "source": [
        "gc.collect()\n",
        "trained_model, losses, validations, predictions = train()\n",
        "torch.save(trained_model, \"/drive/MyDrive/CS-474-Final-Project/sr2.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWSF1itoofE4"
      },
      "source": [
        "# print(losses)\n",
        "# print(validations)\n",
        "# trained_model = torch.load(\"/drive/MyDrive/CS-474-Final-Project/sr2.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcYufikuPDZK"
      },
      "source": [
        "num = 160\n",
        "\n",
        "# Truth Image\n",
        "truth_72 = val_dataset[num][1]\n",
        "truth_72 = truth_72.swapaxes(0, 2).swapaxes(0, 1)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(f'Original Image')\n",
        "plt.imshow(truth_72.cpu().numpy(), cmap = 'gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Blurred Image\n",
        "blur_72 = val_dataset[num][0]\n",
        "blur_72 = blur_72.swapaxes(0, 2).swapaxes(0, 1)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(f'Blurred Image')\n",
        "plt.imshow(blur_72.cpu().numpy(), cmap = 'gray')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Predicted Image\n",
        "pred_72_input = val_dataset[num][0].unsqueeze(0).cuda()\n",
        "pred_72 = trained_model(pred_72_input).squeeze(0).detach()\n",
        "pred_72 = pred_72.cpu().swapaxes(0, 2).swapaxes(0, 1)\n",
        "pred = np.clip(pred_72, 0, 1)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(f'Predicted Image')\n",
        "plt.imshow(pred, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
